{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. One versus all MNIST\n",
    "\n",
    "##### 1.1  Finding optimal hyperparameters for SVC with rbf kernel\n",
    "\n",
    "First the MNIST set is downloaded and split into a training set and a test set. The target set is converted to float.\n",
    "\n",
    "The training and test set are normalized in order to speed up training. (SVCs are sensitive to non normalized data).\n",
    "After that a grid search is performed over a number of hyperparameters in order to find the best pair for a subset\n",
    "of 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search score:  0.8820714285714286\n",
      "{'C': 2, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing as pp\n",
    "\n",
    "# Fetch MNIST\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "X, y = mnist['data'], mnist['target']\n",
    "\n",
    "y = y.astype('float64')  # all y values are chars from the source for some reason..\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=10)\n",
    "\n",
    "# Normalize data to speed up training\n",
    "scaler = pp.StandardScaler().fit(X_train)\n",
    "Xn_train = scaler.transform(X_train)\n",
    "Xn_test = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate SVC\n",
    "rbf = SVC(kernel='rbf', gamma=.001, C=2)\n",
    "\n",
    "# Find good values for C and gamma.\n",
    "C = np.arange(1, 11, 1)\n",
    "gamma = np.arange(0.001, 0.01, 0.001)\n",
    "param_grid = {'C': C, 'gamma': gamma}\n",
    "grid_search = GridSearchCV(rbf, param_grid, scoring='accuracy', n_jobs=10)\n",
    "grid_search.fit(Xn_train[:1000, :], y_train[:1000])\n",
    "\n",
    "# Print training score and best params\n",
    "print(\"Grid search score: \", grid_search.score(Xn_test, y_test))\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1.2 Fit data to training set and calculate score on test set.\n",
    "\n",
    "After training the score will yield an accuracy of 96.79% (0.9679285714285715) on the test set.\n",
    "\n",
    "**PS** If you want to skip the training. Run the cell that loads an already trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9679285714285715\n"
     ]
    }
   ],
   "source": [
    "rbf.fit(Xn_train, y_train)\n",
    "print(\"Accuracy: \", rbf.score(Xn_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "rbf = joblib.load('rbf.model')\n",
    "print(\"Accuracy: \", rbf.score(Xn_test, y_test)) # is bugged sometimes.. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 One versus All\n",
    "\n",
    "This next part compares One-Vs-One and One-Vs-All SVCs.\n",
    "\n",
    "**General Approach**: One-Vs-All works by training one classifier for each class.\n",
    "After training all classifiers one can predict a sample and compare the probabilistic results of the predictors.\n",
    "SVCs are not probabilistic in nature so they need to use Platt Scaling in order to return a result of that nature.\n",
    "\n",
    "The MNIST data set contains target values ranging from 0-9. By modifying the target values to either 1 or 0 for each classifier \n",
    "I can train a classifier to recognize only one number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Make hard copies for later binarization\n",
    "y_0 = np.copy(y_train)\n",
    "y_1 = np.copy(y_train)\n",
    "y_2 = np.copy(y_train)\n",
    "y_3 = np.copy(y_train)\n",
    "y_4 = np.copy(y_train)\n",
    "y_5 = np.copy(y_train)\n",
    "y_6 = np.copy(y_train)\n",
    "y_7 = np.copy(y_train)\n",
    "y_8 = np.copy(y_train)\n",
    "y_9 = np.copy(y_train)\n",
    "\n",
    "# Make classifications binary\n",
    "y_0[y_train != 0] = 1  # special case, inverse column of prediction for correct comparisons\n",
    "y_1[y_train != 1] = 0 # all numbers that arent 1, -> set to 0\n",
    "y_2[y_train != 2] = 0 # repeat\n",
    "y_3[y_train != 3] = 0\n",
    "y_4[y_train != 4] = 0\n",
    "y_5[y_train != 5] = 0\n",
    "y_6[y_train != 6] = 0\n",
    "y_7[y_train != 7] = 0\n",
    "y_8[y_train != 8] = 0\n",
    "y_9[y_train != 9] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models have been trained on Xn_train in advance using the previous results of **gamma=0.001** and **c=2**.\n",
    "Training takes quite a while due to the large training set.\n",
    "\n",
    "Examples:\n",
    "\n",
    "zero_ = SVC(kernel='rbf', gamma=0.001, c=2, Probability=True).fit(Xn_train, y_0)\n",
    "one_ = SVC(kernel='rbf', gamma=0.001, c=2, Probability=True).fit(Xn_train, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load SVCs\n",
    "zero_ = joblib.load('models/0.model')\n",
    "one_ = joblib.load('models/1.model')\n",
    "two_ = joblib.load('models/2.model')\n",
    "three_ = joblib.load('models/3.model')\n",
    "four_ = joblib.load('models/4.model')\n",
    "five_ = joblib.load('models/5.model')\n",
    "six_ = joblib.load('models/6.model')\n",
    "seven_ = joblib.load('models/7.model')\n",
    "eight_ = joblib.load('models/8.model')\n",
    "nine_ = joblib.load('models/9.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### 1.3 Predictions\n",
    "\n",
    "This next section predicts all rows of the test set and adds it to a list of predictions. The sum of errors\n",
    "and accuracy is calculated to **96.89%**\n",
    "\n",
    "#### Conclusions\n",
    "\n",
    "Accuracy wise the One-Vs-All approach is slightly better with a 0.10% difference but has a huge computational\n",
    "disadvantage in that it has to train 10 different models compared to One-Vs-One.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9689285714285715\n",
      "Errors: 435\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.array([])\n",
    "# Predict all rows\n",
    "\n",
    "for row in Xn_test:\n",
    "    probs = np.array([zero_.predict_proba(row.reshape(1, -1))[0, 0]]) # special case, probability column is inverted\n",
    "    probs = np.append(probs, [one_.predict_proba(row.reshape(1, -1))[0, 1]])\n",
    "    probs = np.append(probs, [two_.predict_proba(row.reshape(1, -1))[0, 1]])\n",
    "    probs = np.append(probs, [three_.predict_proba(row.reshape(1, -1))[0, 1]])\n",
    "    probs = np.append(probs, [four_.predict_proba(row.reshape(1, -1))[0, 1]])\n",
    "    probs = np.append(probs, [five_.predict_proba(row.reshape(1, -1))[0, 1]])\n",
    "    probs = np.append(probs, [six_.predict_proba(row.reshape(1, -1))[0, 1]])\n",
    "    probs = np.append(probs, [seven_.predict_proba(row.reshape(1, -1))[0, 1]])\n",
    "    probs = np.append(probs, [eight_.predict_proba(row.reshape(1, -1))[0, 1]])\n",
    "    probs = np.append(probs, [nine_.predict_proba(row.reshape(1, -1))[0, 1]])\n",
    "    index = probs.argmax()\n",
    "    y_pred = np.append(y_pred, index)\n",
    "\n",
    "\n",
    "errors = np.sum(y_pred != y_test)\n",
    "print(\"Accuracy: \", 1- (errors/len(Xn_test)))\n",
    "print(\"Errors:\" , errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
